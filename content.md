生成式 AI 與資訊安全的雙刃劍
簡介

生成式人工智慧（Generative AI）是近年來最具顛覆性的技術之一。它能自動生成文字、圖像、程式碼、語音甚至影片，大幅提升工作效率與創意產出。然而，隨著生成式 AI 從實驗階段走向主流商業應用（尤其在 2024 ～ 2025 年間），企業投入前所未有的資源，也引發對其在資安領域風險與效益的激烈討論。在資訊安全產業中，AI 正扮演著雙刃劍的角色：一方面，它讓防禦者能以前所未有的速度與規模偵測威脅並回應事件；但另一方面，攻擊者也利用 AI 打造難以識破的網路釣魚、深偽內容與自動化攻擊工具，顯著提高攻擊的隱蔽性與有效性。

因此，生成式 AI 帶來的挑戰不僅是技術層面，更引發了一場關於信任、倫理與安全架構的革命。未來的資訊安全不再只是部署防火牆和設定密碼，而是在於我們如何與 AI 共存——讓 AI 成為守護者而非入侵者。這需要我們重新思考安全策略，將 AI 納入整體防禦體系，同時建立相應的治理與道德規範。

（註：以下將從正面與負面兩大面向，分析生成式 AI 對資訊安全的影響。）

正面影響：AI 強化防禦與偵測能力

智慧化威脅分析：生成式 AI 能夠分析海量的系統日誌、網路流量和使用者行為資料，主動找出可疑的異常模式。例如它可以即時偵測出異常的登入活動或數據傳輸。相較於傳統根據已知特徵碼比對惡意程式，AI 藉由學習「正常」行為來發現細微異常，甚至捕捉未知的零時差攻擊或多變的惡意程式。研究顯示，在高風險環境下引入 AI 後，威脅偵測率可提高到 98%。這種以異常行為為基礎的偵測方式，大幅強化了資安人員發現威脅的能力。

**自動化資安工作：**AI 可以自動執行許多原本需人工的繁瑣資安任務，減輕安全團隊的工作負擔。例如，生成式 AI 能自動生成滲透測試報告，快速彙整系統弱點並提出修補建議；也能模擬各種攻擊情境，協助安全人員進行紅隊演練和弱點評估。事實上，現在已出現利用機器學習的自動化滲透測試工具（如開源的 DeepExploit 或商用的 NodeZero），可自行進行情報蒐集、威脅建模、弱點分析與利用等步驟。透過這類工具，安全團隊能以更快、更經濟的方式測試防禦能力，將人力資源集中於更需創意與判斷力的工作。

**即時防詐與社交工程防禦：**生成式 AI 也可用於識別詐騙訊息和社交工程攻擊。透過自然語言處理能力，AI 可以分析郵件或訊息內容的語氣和上下文，判斷其中是否包含釣魚詐騙的跡象。例如傳統反垃圾郵件系統著重於關鍵字或可疑連結，但現代 AI 模型能檢視郵件的整體語意與發件人行為模式，發現細微的可疑之處（如寄件者身份與平時風格不符等）並標記警示。對於偵測 Deepfake 假影片或合成語音詐騙，雖然這仍是持續對抗的領域，但也已有 AI 工具著手分析影音真偽特徵，試圖及早攔截深偽攻擊。總體而言，AI 為釣魚郵件過濾、假訊息偵測提供了更智慧的即時防護，有助於保護使用者免受欺騙。

**輔助決策與即時響應：**結合生成式 AI 的安全系統能協助人員在最短時間內做出最佳反應。AI 可以即時匯總多種威脅情報，提供清晰的事件描述與處置建議，縮短從發現威脅到採取行動的時間。尤其在重大安全事件中，AI 產生的自然語言報告和分析摘要，能讓決策者迅速了解整體狀況並採取對策。例如：微軟的「Security Copilot」是一款基於 GPT-4 的大型語言模型的資安助理，它能快速整理來自全球每日 65 兆條訊號的威脅資訊，在幾分鐘內生成入侵過程摘要與報告，提供逐步指引來協助調查與因應。💡 **例子：**Security Copilot 能以自然語言回答安全人員的查詢，並自動產生事件報告供不同受眾閱讀，協助分析師快速了解駭客入侵的途徑和影響範圍。透過這類 AI 輔助決策工具，資安團隊的反應速度和處置品質都獲得顯著提升。

負面影響：AI 成為攻擊與詐騙的利器

**社交工程更難分辨：**攻擊者利用生成式 AI 打造出高度逼真的網路釣魚郵件、語音和影片，使人們越來越難以分辨真偽。過去我們可透過拼寫錯誤、用語怪異等來識破釣魚信件，但如今 AI 生成的郵件在語法和語氣上幾乎天衣無縫，甚至可個人化內容來瞄準受害者。**釣魚攻擊的數量與成功率因 AI 劇增：**統計顯示，與生成式 AI 相關的釣魚攻擊激增了 1265%，部分報告甚至指稱增幅超過 40 倍；更危險的是，AI 撰寫的釣魚郵件平均有 54%的收件人會點擊連結，相較傳統手法僅約 12%的點擊率，有巨幅提升。此外，透過生成式 AI，攻擊者只需幾分鐘就能寫出精心設計的欺詐郵件（人工可能需數小時），攻擊成本因此降低約 95%，使大規模客製化詐騙變得輕而易舉。更棘手的是，AI 不只用於文字詐騙，還能生成栩栩如生的聲音與影片（所謂 Deepfake 深偽技術），讓假訊息更具說服力。在 2025 年第一季就記錄了 179 起深偽詐騙事件，數量已超過整個 2024 年，比前年同期增加了 19%。
。攻擊者甚至能即時偽造主管的視訊會議畫面和聲音來行騙。💡 例子：2024 年全球某工程顧問公司發生一起震驚業界的深偽詐騙事件：詐騙者偽裝成該公司財務長发送緊急郵件並安排線上會議，會議中出現的財務長和同事全是 AI 生成的假影像，騙取財務人員信任，最終讓其在短時間內分 15 次匯出共約 2,560 萬美元給攻擊者
。這類案例顯示，AI 讓傳統的社交工程詐騙更具迷惑性，攻擊者不再需要入侵系統，就能透過「技術強化的社交工程」直接攻破人性的防線
deepstrike.io
。

惡意程式生成門檻下降：生成式 AI 的普及大大降低了製作惡意程式和攻擊工具的門檻。以往只有熟練的駭客才能編寫惡意程式碼，但現在即使是缺乏經驗的攻擊者，也可以利用 AI 模型自動產生滲透腳本或病毒程式。
ibm.com
2023 年甚至出現了在暗網販售的惡意聊天機器人（如“WormGPT”和“FraudGPT”），這些經過修改的生成式模型移除了道德限制，專門用於違法用途。研究人員發現，這類工具能輕易生成極具說服力的商業電郵詐騙（BEC）訊息等惡意內容，相當於提供了*「犯罪即服務」*的平台
deepstrike.io
。此外，AI 也被用來開發更難偵測的惡意軟體。例如所謂多態惡意程式（Polymorphic Malware），能不斷修改自身程式碼特徵以逃避防毒軟體。AI 進一步強化了這種變形能力，某些惡意軟體甚至可以每 15 秒自動生成不同變種。據統計，2025 年有高達**76.4%**的網路釣魚攻擊 campaign 內含多態惡意程式，而超過 70%的重大資安事件涉及某種形式的多態惡意碼。更有現成的 AI 惡意程式生成套件在黑市以不到 50 美元價格出售，使大规模部署惡意攻擊的技術門檻前所未見地低。

AI 模型本身的風險：生成式 AI 模型也可能成為攻擊目標，出現「被駭的 AI」情形。攻擊者可以對 AI 模型進行資料投毒（Data Poisoning），在模型訓練階段刻意注入不良或惡意的訓練資料，導致模型學到偏差內容。例如，一個被投毒的安全 AI 可能被訓練得信任特定的惡意 IP 位址，或忽略某類攻擊的跡象。另外還有提示注入（Prompt Injection）攻擊：攻擊者精心設計特殊的輸入（提示），誘使生成式模型忽略原有的安全限制，按照攻擊者指令產生不當內容或洩露機密信息。例如，有知名的「做任何事」(DAN) 提示攻擊曾一度成功繞過 ChatGPT 的安全限制。總之，AI 模型可能被迫生成有害內容、洩露敏感資料或偏離原本設計目的。這不但危及使用者與機密資訊，長遠而言也侵蝕人們對 AI 系統的信任。如果防禦方過度依賴 AI 而 AI 本身被暗中操控，後果將不堪設想。

資訊偽造與信任崩壞：生成式 AI 能大量產出以假亂真的資訊，從文字消息到影音內容，對資訊真實性造成前所未有的衝擊。深偽技術讓不實訊息更加難辨，可能被不法分子用來製造假新聞、假證據，進而影響輿論與民主程序。例如，人們一度擔憂在 2024 年各國大選中，深偽影片和 AI 生成假新聞會引發資訊亂象甚至顛覆選情——雖然最終未出現大規模事件，但這種潛在威脅已令全球安全單位高度戒備
weforum.org
。更常見的，是 AI 生成的不實內容在網路上廣泛流傳，讓公眾難以判斷訊息來源可靠與否。當一般人開始懷疑眼見耳聞的一切時，「信任」本身就成了一種稀缺資源。深偽內容引發的誤導可能造成社會混亂，而各國政府也將如何確保資訊可信度視為新的安全課題。此外，企業也面臨品牌信譽被假資訊抹黑的風險。因此，隨著生成式 AI 技術普及，全球正進入一個真偽難分的時代，如何驗證數位內容的真實性，防範大眾被大规模誤導，已成為資訊安全領域亟待解決的難題。

結論

生成式 AI 正迅速重塑資安版圖，我們既不能忽視其帶來的強大防禦助力，也不可低估其被惡意利用的破壞力。在這場 AI 軍備競賽中，防守方唯有積極運用 AI 同時謹慎治理 AI，才能立於不敗之地。未來的資訊安全策略將強調人機協同：一方面透過 AI 強化預警偵測、加速響應；另一方面建立健全的 AI 治理架構和倫理指南（如採用 NIST 人工智慧風險管理框架等）來約束 AI 的使用。事實上，研究發現大多數因 AI 引發的資安事故，往往是缺乏適當管控所致——97%的 AI 相關安全事件發生在缺乏良好治理與權限控管的情境中。這提醒我們：科技必須與制度並行。組織應制定明確的 AI 使用政策，防範「陰影 AI」（未經授權部署的 AI）帶來的隱患；同時培養員工的 AI 素養，打造對深偽和 AI 詐騙具備警覺性的文化。

總而言之，生成式 AI 帶來的衝擊將長久地改變資訊安全的風貌。我們不能也不應試圖阻擋 AI 的進步，而是要以「以 AI 制 AI」的思維，善用 AI 來對抗 AI 驅動的威脅。在強化技術防禦的同時，更要重建數位時代的信任機制，確保 AI 技術的發展符合人類的道德與安全底線。唯有如此，我們才能讓 AI 成為守護者而非入侵者，在新興的智慧安全架構中立於主動。藉由持續的創新、防禦與治理並舉，我們有望迎來一個人類與 AI 共存共榮的安全新時代。
